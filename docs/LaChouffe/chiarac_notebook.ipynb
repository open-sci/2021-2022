{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chiara's journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents:\n",
    "\n",
    "* [Research questions](#researchQ)\n",
    "\n",
    "* [Diary](#diary)\n",
    "    * [Week one](#first-week)\n",
    "\n",
    "    * [Week two](#second-week)\n",
    "\n",
    "        * [Literature research](#literature-research)\n",
    "            * [Research strategy](#research-strategies) \n",
    "\n",
    "        * [Data Management Plan](#DMP)\n",
    "            * [Primary rules](#DMP-rules)\n",
    "            * [Repository - Zenodo](#DMP-repository)\n",
    "            \n",
    "        * [Protocols](#protocols)\n",
    "        \n",
    "        * [Useful resources](#week2-resources)\n",
    "    * [Week three](#third-week)\n",
    "        \n",
    "* [Research on the state-of-the-art](#sota)\n",
    "* [DMP versions](#DMP-versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research questions <a class=\"anchor\" id=\"researchQ\"></a>\n",
    "\n",
    "- How many articles published in the open access journals in DOAJ are included in Crossref? \n",
    "- How many of these articles include information about their reference lists? \n",
    "- How many references have a DOI specified? \n",
    "- How many of these DOIs have been specified by the publishers? \n",
    "- And how many by Crossref?\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diary <a class=\"anchor\" id=\"diary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Week one <a class=\"anchor\" id=\"first-week\"></a>\n",
    "##### 21/03/2022 - 27/03/2022\n",
    "#### Abstract \n",
    "\n",
    "Found an interesting article on the importance of evaluaing accessibility of open data.\n",
    "\n",
    "*It is thus an unfortunate irony that open-science practices are not equally accessible to all scientists. We often lament the paywalls that make research inaccessible to the reader, but we often do not lament the paywalls, in the form of specific barriers to openness, that prevent many scientists from sharing their work in the first place. Accessibility does not happen by accident, but requires us to intentionally evaluate our work, practices, and organizations to ensure that they are accessible. This evaluation is a constant iterative process—and not a one-off decision—to question whether our new praxis has retained the existing structural inequalities of the one before it.*\n",
    "\n",
    "(taken from https://www.americanscientist.org/article/open-science-isnt-always-open-to-all-scientists).\n",
    "\n",
    "* Purpose: \n",
    "The purpose of a study like ours could be to evaluate accessibility and transaparency of information at the state-o-the-art.\n",
    "\n",
    "Why is it worth making research on DOIs? What is the weight of tools like DOJA in the schoolar publishing field? What is the state of curation of articles references? Inquire overlap between DOJA and Crossref, \n",
    "\n",
    "*Davides's text to start with:*\n",
    "\"This research paper inquires how much overlap there is between the articles from Open Access journals in DOAJ and Crossref. Furthermore, we scouted the availability of these articles’ reference lists, the presence of IDs such as DOIs and the entities responsible for their specification. This analysis was carried out by querying the DOAJ and Crossref’s APIs, creating a dataset combining the two. The results reveal that only a small portion(?) of the articles on the DOAJ have relevant information in Crossref, but that most of them have a relevant id (?). Also, most of the information was fiven by Crossref itself(?). This is information is relevant because previous studies were not able to organise this kind of information in an optimal way.\"\n",
    "\n",
    "#### Research on DOJA and Crossref\n",
    "\n",
    "##### Crossref\n",
    "\n",
    "Crossref is a not-for-profit membership association which aims at promoting the development and cooperative use of new and innovative technologies to speed and facilitate scientific and other scholarly research\n",
    "Crossref is one of the ten International DOI registration agencies, and allows its members to register the DOIs of their publications\n",
    "Each DOI registered in the Crossref system is associated with a URL to the publication’s webpage and accompanied with the metadata of the publications\n",
    "Crossref provides a REST API to retrieve data about the entities\n",
    " \n",
    "\n",
    "##### DOJA\n",
    "The Directory of Open Access Journals (DOAJ, https://doaj.org/) is an independent index containing more than 17,500 peer-reviewed, open access journals covering all areas of science, technology, medicine, social sciences, arts and humanities\n",
    "\n",
    "In addition to journals, it also **contains basic metadata of more than 7 million journal articles by several publishers from 130 countries**.\n",
    "\n",
    "When using DOJA's **API** note that not all fields will be available on all records. I did some research on their documentation and the internet to understand ho to manage curl requests in python.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Week two <a class=\"anchor\" id=\"second-week\"></a>\n",
    "##### 28/03/2022 - 03/04/2022\n",
    "\n",
    "#### Literature research <a class=\"anchor\" id=\"literature-research\"></a>\n",
    "##### Research strategy <a class=\"anchor\" id=\"research-strategies\"></a>\n",
    "\n",
    "**Research by:** Keywords, available literature from course repository\n",
    "\n",
    "**Sources:** [Google Scholar](https://scholar.google.com/), [The Lens](https://www.lens.org/) maybe for a first reserch round in order to get a general grasp on the state-of-the art, and [ResearchRabbit](https://researchrabbitapp.com/) as main tool to perform a more granular research on our study field.\n",
    "\n",
    "**Resources storage** \n",
    "The retrieved literature will be collected and stored on Zotero (which can be connected with a ResearchRaabit work space)\n",
    "\n",
    "\n",
    "\n",
    "#### Data mamagement plan <a class=\"anchor\" id=\"DMP\"></a>\n",
    "##### Primary rules <a class=\"anchor\" id=\"DMP-rules\"></a>\n",
    "\n",
    "<!-- The following is maybea bit ot of out of contex fo us  -->\n",
    "1. **Determine the Research Sponsor Requirements**\n",
    "\n",
    "2. **Identify the Data to Be Collected:** \\\n",
    "by types, sources, volume and data/file formats\n",
    "\n",
    "3. **Define How the Data Will Be Organized:**\\\n",
    "Small data volumes can be managed with Excel, while larger data volumes and usage constraints may require the use of relational database management systems (RDBMS) for linked data tables like ORACLE or mySQL, or a Geographic Information System (GIS) for geospatial data layers like ArcGIS, GRASS, or QGIS.\\\n",
    "Therefore, in drafting a DMP, it is most helpful to initially focus on the types and, possibly, names of the products that will be used.\n",
    "\n",
    "Database 1  - journals del doaj - file json\n",
    "Database 2 - csv con dati disaggregati dei journals del doaj su crossref\n",
    "Database 3 - csv con dati aggregati con presenza citazioni\n",
    "\n",
    "\n",
    "4.  **Explain How the Data Will Be Documented**\n",
    "\n",
    "    * First, identify the types of information that should be captured to enable discovery, access, interpretation, useage, and citation of your data. \n",
    "    * Second, determine whether there is a community-based metadata schema or standard, that can be adopted. E.g., variations of *Dublin Core Metadata Initiative Abstract Model*, *ISO* (International Organization for Standardization) for geospatial data. A specific metadata content standard will be recommended by a target data repository, archive, or domain professional organization. \n",
    "    * Third, identify software tools that can be employed to create and manage metadata content (e.g., Metavist, Morpho). Text files (e.g., readme.txt) that include the relevant metadata can be included as headers to the data files.\n",
    "\n",
    "    A sound documentation strategy can be based on these three steps.\n",
    "\n",
    "       \n",
    "5. **Describe How Data Quality Will Be Assured:** \\\n",
    "It is good practice to describe the QA/QC measures that you plan to employ in your project. Such measures may encompass training activities, instrument calibration and verification tests, double-blind data entry, and statistical and visualization approaches to error detection. Simple graphical data exploration approaches (e.g., scatterplots, mapping) can be invaluable for detecting anomalies and errors.\\\n",
    "\n",
    "6. **Present a Sound Data Storage and Preservation Strategy:**\\\n",
    "Many universities and organizations also host institutional repositories, and there are numerous general science data repositories such as [Dryad](http://datadryad.org/), [figshare](http://figshare.com/), and [Zenodo](http://zenodo.org/).\n",
    " \n",
    "    * How long will the data be accessible?\n",
    "    * How will data be stored and protected over the duration of the project\n",
    "    * How will data be preserved and made available for future use?\\\n",
    "    \n",
    "    The above questions must be answered in order to decide ho to presere datya in a relient way.\n",
    "\n",
    "7. **Define the Project’s Data Policies**:\\\n",
    "Data policies include:\n",
    "    * *Licensing or sharing arrangements* that pertain to the use of preexisting materials\n",
    "    * *Plans for retaining, licensing, sharing, and embargoing* (i.e., limiting use by others for a period of time) data, code, and other materials\n",
    "    * *Legal and ethical restrictions* on access and use of human subject and other sensitive data.\n",
    "\n",
    "    If preexisting materials, such as data and code, are being used, identify and include a description of the relevant licensing and sharing arrangements in your DMP. Explain how third party software or libraries are used in the creation and release of new software.\n",
    "    Whenever possible, apply standard rights waivers or licenses, such as those established by [Open Data Commons (ODC)](http://opendatacommons.org/licenses/pddl/summary/) and [Creative Commons (CC)](http://creativecommons.org/ ), that guide subsequent use of data and other intellectual products. The CC0 license and the ODC Public Domain Dedication and License, for example, promote unrestricted sharing and data use.\\\n",
    "    Explain how human subject and other sensitive data will be treated (e.g., see http://privacyruleandresearch.nih.gov/ for information pertaining to human health research regulations set forth in the US Health Insurance Portability and Accountability Act).\n",
    "\n",
    "8. **Describe How the Data Will Be Disseminated**\\\n",
    "State when, how, and what data products will be made available. Generally, making data available to the greatest extent and with the fewest possible restrictions at the time of publication or project completion is encouraged.\n",
    "\n",
    "9. **Assign Roles and Responsibilities:**\\\n",
    "Roles may include data collection, data entry, QA/QC, metadata creation and management, backup, data preparation and submission to an archive, and systems administration.\\\n",
    "Assign a project team member to revise the plan, reflecting any new changes in protocols and policies. It is good practice to track any changes in a revision history that lists the dates that any changes were made to the plan along with the details about those changes, including who made them.\n",
    "\n",
    "<!-- The following is maybea bit ot of out of contex fo us  -->\n",
    "10. **Prepare a Realistic Budget**\n",
    "\n",
    "#### Repository - Zenodo <a class=\"anchor\" id=\"DMP-repository\"></a>\n",
    "\n",
    "After each submission, a persistent digital object identifier (DOI) is associated to the resource, which makes the stored items easily citable.\n",
    "\n",
    "#### Useful resorces <a class=\"anchor\" id=\"week2-resources\"></a>\n",
    "\n",
    "[DMP openaire website user-guide](https://argos.openaire.eu/user-guide)\\\n",
    "[10 simple rules  for creating a good DMP by William K. Michen](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004525)<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Week three <a class=\"anchor\" id=\"third-week\"></a>\n",
    "##### 04/04/2022 - 10/04/2022\n",
    "\n",
    "#### Literature research \n",
    "\n",
    "#### 1. [A Bibliometric Study of Open Access Sociology Journals in DOAJ](https://www.ijrls.in/wp-content/uploads/2021/06/ijrls-1410.pdf)   \n",
    "##### Abstract\n",
    "The present study reports the findings of bibliometric analysis of open access journals available in the subject of ‘Sociology’ indexed in directory of open access journals (DOAJ). \n",
    "\n",
    "##### Methodology\n",
    "The data for the study were analyzed using MS Excel spreadsheets to achieve the objectives of the study. The details were entered to Excel spreadsheets as title of the journals, Publication format, subject discipline, year of added, country of publication and language of the journal.\n",
    "\n",
    "#### 2. [The journey of DOAJ from 2002 to 2019: The developing supplier of scholarly literature](https://www.researchgate.net/profile/Nilakshi-Sharma/publication/343987442_The_journey_of_DOAJ_from_2002_to_2019_The_developing_supplier_of_scholarly_literature/links/5f840f5d299bf1b53e20d029/The-journey-of-DOAJ-from-2002-to-2019-The-developing-supplier-of-scholarly-literature.pdf)    \n",
    "##### Abstract\n",
    "Present study was made to analyze each and every essential aspects of\n",
    "DOAJ from its beginning to 2019. Till 04-05-2019\n",
    "\n",
    "##### Objectives\n",
    "1. To observe the country wise contribution of journals to DOAJ and rank top 30\n",
    "contributing countries.\n",
    "2. To observe the subject wise distribution of DOAJ indexed journals.\n",
    "3. To observe the publisher wise distribution of DOAJ indexed journals and makes a list\n",
    "of top contributing publishers.\n",
    "4. Study of different factors of journals like year of starting online open access, year and\n",
    "month of adding to DOAJ, time taken between submission and publication of articles,\n",
    "Review process used by the journal, permission of unrestricted reuse in compliance\n",
    "with BOAI, copyright, publishing rights, availability of DOAJ Seal, file format used\n",
    "by the journal.\n",
    "5. To find out the no. of journal provides download statistics, full-text crawl permission,\n",
    "Journal waiver policy, Article Processing Charges (APCs), and Machine-readable CC\n",
    "licensing information.\n",
    "\n",
    "##### Methodology\n",
    "For the study, a well devised methodology was applied. Directory of Open\n",
    "Access Journal (DOAJ) itself was the source data for the study. Metadata database of all the\n",
    "DOAJ indexed journals were accessed from its website (http://www.doaj.org) on 04-05-2019.\n",
    "Then the Journals which are providing its full text in English Language only, those 6003 journals\n",
    "are extracted from the whole database using Microsoft excel software .Then relevant data (title,\n",
    "country, publisher, subject, year, APC,DOAJ seal etc.) were filtered and analyzed with the help\n",
    "Microsoft excel using some mathematical formulae. Analyzed data are presented with different\n",
    "tables, graphs and figures. \n",
    "\n",
    "#### 3. [A Bibliometric Study of Open Access Sociology Journals in DOAJ](https://direct.mit.edu/qss/article/2/1/20/97574/Large-scale-comparison-of-bibliographic-data)   \n",
    "##### Abstract\n",
    "In this paper, we present a large-scale document-level comparison of five major bibliographic data sources: Scopus, WoS, Dimensions, Crossref, and Microsoft Academic. Our focus is on differences between the data sources in the coverage of documents. In addition, we also study differences in the completeness and accuracy of citation links. We consider only scientific documents, such as journal articles, preprints, conference proceedings papers, books, and book chapters, in our analysis. Some data sources also provide data on other types of entities. Dimensions, for instance, offers data on grants, data sets, clinical trials, patents, and policy documents. Likewise, the WoS platform provides data on data sets and patents. While these data can be of great value, they fall outside the scope of our analysis.\n",
    "\n",
    "##### Sources \n",
    "\n",
    "**Crossref** (Hendricks et al., 2020) is a special case. It is a registration agency for DOIs. When a scientific publisher works with Crossref to register a DOI for a document, the publisher provides metadata for this document to Crossref. This metadata is then made openly available by Crossref (with the possible exception of the reference list, for which the publisher determines whether it is made openly available or not). In this way, Crossref has become a bibliographic data source that is of significant interest for bibliometric analyses. The completeness and the quality of the data available in Crossref depend on what publishers provide to Crossref. Crossref itself does not actively collect and enrich data.\n",
    "\n",
    "##### Data manipulation\n",
    "**Preprocessing**\n",
    " In the case of publication years and volume, issue, page, and article numbers, the preprocessing process retains only numerical characters. All other characters are discarded. The preprocessing process also splits author names in Microsoft Academic into first and last names. In the other data sources, this has already been done by the data provider. In the matching procedure, we treat the first character of the first name of an author as the author’s first initial\n",
    "\n",
    " **Matching**\n",
    "\n",
    "#### Notes\n",
    "\n",
    "Doja doesn not say at least on the page where the DOI was registered, thus we are going to need a matching algorithm for finding them\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Week four <a class=\"anchor\" id=\"fourth-week\"></a>\n",
    "##### 12/04/2022 - 19/04/2022\n",
    "\n",
    "#### Meeting\n",
    "We decided to use DOAJ's dumps as data samples for creating our software. We will have un csv for the dump with journals and one with their articles \n",
    "We decided to create two datasets:\n",
    "1. DOJA journals = doaj data dump enriched with number of aricles on crossref [ISSN, tot art, crossref art]\n",
    "2. Citations = [citing, cited, DOIagency]\n",
    "\n",
    "Agencies can be found with api requesto or looking into the DOI string \n",
    "Updated abstract\n",
    "\n",
    "workflow creation on protocols.io "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET JOURNALS ISSN\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "journals_data = None\n",
    "with open('/Users/chiara/Desktop/doaj_journal_data_2022-04-07/journal_batch_1.json', 'r') as f:\n",
    "    data = f.read()\n",
    "    journals_data = json.loads(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fro doaj documentation we know that the issn identifier of each article corresponds to their journal's identifier. We will now start gathering each journal's issn from our dump and ut them in a dictionary that is needed for forming the later request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = dict()\n",
    "for journal in journals_data:\n",
    "\n",
    "    if 'pissn' in journal['bibjson']:\n",
    "\n",
    "        codes[journal['bibjson']['pissn']]='pissn'\n",
    "    \n",
    "    elif 'eissn' in journal['bibjson']:\n",
    "        codes[journal['bibjson']['eissn']]='eissn' \n",
    "    \n",
    "    else:\n",
    "\n",
    "\n",
    "        print('both missing'), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMP versions <a class=\"anchor\" id=\"DMP-versions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 0**: https://doi.org/10.5281/zenodo.6411449\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
