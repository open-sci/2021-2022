{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75da9b0e",
   "metadata": {},
   "source": [
    "# Open Science project 2021-22 personal diary\n",
    "## Team Don't lock up\n",
    "\n",
    "## Research questions:\n",
    "- Which is the coverage, in terms of citations, of the open access journals in DOAJ according to the data available in OpenCitations? \n",
    "- How many citations DOAJ journals receive? \n",
    "- How many citations DOAJ journals do? \n",
    "- How many of these citations involves articles published in open access journals as both citing and cited entities? \n",
    "- What is the trend (increasing? decreasing?) in time of the availability of such citations involving journals in DOAJ?\n",
    "\n",
    "## 28-03-22 and 29-03-22 First research on the topics\n",
    "After agreeing on team members, team name and research questions, the first task at hand is to prepare the abstract for our research, following [Emerald Publishing recommendations](https://www.emeraldgrouppublishing.com/how-to/authoring-editing-reviewing/write-article-abstract).\n",
    "The abstract will have to feature the Purpose of our research, the Study design/methodology/approach, our Findings and the Originality/Value of our research. \n",
    "\n",
    "I did some research on [OpenCitations' COCI](https://opencitations.net/index/coci), [Crossref](https://www.crossref.org/documentation/) and [DOAJ](https://doaj.org/) and try to reformulate the research questions in order to find relevant points to bring to the first team meeting, which we agreed would be before the next lecture, 30/03/22 morning.\n",
    "\n",
    "We will need to address the research questions about DOAJ journals citing and cited articles according to OpenCitations, citations with citing and cited DOIs published in open access journals and finally the presence of trends over time of the availability of such citations in DOAJ journals. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6208869",
   "metadata": {},
   "source": [
    "## 30-03-22 First version of the abstract\n",
    "\n",
    "We had a team meeting to write the abstract together. We reflected on the research questions and came up with a first version, to be updated as we continue the project. \n",
    "\n",
    "---\n",
    "### Purpose\n",
    "Citations are always important in scientific research. In the domain of Open Science, the Initiative for Open Citations promotes the availability of data on citations with OpenCitations. *As of March 2021, the fraction of publications with open references has grown from 1% to 87% out of 54.2 million articles with references deposited with Crossref.* ([i4oc.org](https://i4oa.org/))\n",
    "\n",
    "Our goal is to find out \n",
    "* about the coverage of articles from open access journals in DOAJ journals as citing and cited articles,\n",
    "* how many citations do DOAJ journals receive and do, and how many of these citations involve open access articles as both citing and cited entities,\n",
    "* as well as the presence of trends over time of the availability of citations involving articles published in open access journals in DOAJ journals.\n",
    "\n",
    "### Approach\n",
    "We used the data available in OpenCitations, such as the Index of Crossref open DOI-to-DOI citations (COCI). We used the REST API to use the data available in DOAJ and OpenCitations.\n",
    "\n",
    "### Findings\n",
    "We revealed that the coverage of open citations in DOAJ can be improved, and that many of these citations involve articles which are not in open access, but that the trend is still increasing.\n",
    "\n",
    "### Originality\n",
    "Our research is valuable for the open access domain, by revealing trends on the data of DOAJ. In the future more questions can be build on our research.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912221b",
   "metadata": {},
   "source": [
    "## 06-04-2022 First version of the DMP\n",
    "We held a team meeting on Teams to fill the Data Management Plan together, using the templates offered by Argos. After the general information about the DMP, we filled out information about two datasets, one for the software that we will develop and one for the data that we will produce. \n",
    "\n",
    "It was relatively difficult to answer all of the questions since we have not actually started the research, but it allowed us to think about whihc specific aspects to take into account once we start the project in order to make it compliant to Open Science standards and requirements. \n",
    "We first started filling the Horizon Europe template, but due to some bug that didn't allow us to submit it, we had to use Horizon 2020 and refill it entirely.\n",
    "\n",
    "The vesion 0 of the DMP can be found at [this address](https://doi.org/10.5281/zenodo.6417368)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6613217",
   "metadata": {},
   "source": [
    "## 10-04-2022 Protocol\n",
    "We had a meeting on Teams to talk about the protocol, since we had to produce the first verison of it before the class on Wednesday. We first tried to sketch the protocol with images to figure out how to precisely answer the research questions one by one. \n",
    "\n",
    "When trying to understand exactly how to handle the research, we looked into the possibilities offered by OpenCitations Indexes: mainly the SPARQL endpoint and the API. \n",
    "\n",
    "We tried to produce drafts of SPARQL queries that would answer our questions.\n",
    "### Which is the coverage, in terms of citations, of the open access journals in DOAJ according to the data available in OpenCitations?\n",
    "- *How many citations DOAJ journals receive?*\n",
    "\n",
    "We will query on all citations inside COCI and CROCI, filtering only those with DOAJ journals as cited.\n",
    "- *How many citations DOAJ journals do?*\n",
    "\n",
    "We will query on all citations inside COCI and CROCI, filtering only those with DOAJ journals as citing.\n",
    "- *How many of these citations involves articles published in open access journals as both citing and cited entities?*\n",
    "\n",
    "Combining the two previous dataframes, we extract only those with open access journals as citing and cited entities. We discovered that with the API for COCI, there is a way to check in the metadata for an article if there is an open access entry for it. \n",
    "- *What is the trend (increasing? decreasing?) in time of the availability of such citations involving journals in DOAJ?*\n",
    "\n",
    "We will sort the previous list by year and study the presence or absence of a trend. Eventually we will extract a plot or other visualizing tool to present the results.\n",
    "\n",
    "We then used [protocols.io](protocols.io) to write the first version of the protocol (a very basic draft), available at [this link](dx.doi.org/10.17504/protocols.io.n92ldz598v5b/v1) in the Open Science 2021-22 repository created by prof. Silvio Peroni. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c12a5",
   "metadata": {},
   "source": [
    "## 15-04-22 and 16-04-22 Research in order to revise the protocol\n",
    "Prof. Peroni asked us to refine our protocol before evaluation by the other team and asked us to be more precise. \n",
    "\n",
    "I did a little research on [Google Scholar](https://scholar.google.com/) trying to find papers that would be useful for our research, here are two of them that I found relevant: \n",
    "- David J. Solomon, Mikael Laakso, Bo-Christer Björk, \"A longitudinal comparison of citation rates and growth among open access journals\",Journal of Informetrics,Vol. 7, Issue 3, 2013, pp. 642-650. [https://doi.org/10.1016/j.joi.2013.03.008](https://doi.org/10.1016/j.joi.2013.03.008)\n",
    "- Chifumi Nishioka, Michael Färber, \"Evaluating the Availability of Open Citation Data\", BIRNDL@ SIGIR. 2019. p. 123-129. [link](http://ceur-ws.org/Vol-2414/paper13.pdf)\n",
    "\n",
    "I also looked more into COCI's API and DOAJ's API and dump, trying to understand how we will be able to retrieve data useful for our research. I downloaded the [DOAJ public data dump](https://doaj.org/docs/public-data-dump/) to understand which metadata about the journals is available. I discovered that the ISSN and EISSN of the journals may be the best way to filter then the citations in OpenCitations, because for each article DOI, the metadata holding the information of the source of the article (i.e. the ISSN of the journal where it was published) can be retrieved through the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f026a5",
   "metadata": {},
   "source": [
    "## 19-04-22 and 20-04-22 Revision of the protocol (v.2)\n",
    "We had a team meeting on Teams to talk about the protocol and make the steps more precise. \n",
    "I shared the information that I found about the DOAJ dump and COCI API and we discussed how we would retrieve data for our research, we are still indecisive about using the SPARQL endpoint or the API of OpenCitations and plan on asking some information to prof. Peroni about it. \n",
    "We defined better each step of the protocol, explaining which formats of data we will use at each step and probably the method used to do so, even though we are not entirely fixed upon that since we haven't started developing the software yet. \n",
    "\n",
    "We will mostly use JSON and CSV, and probably also use [pandas library](https://pandas.pydata.org/) of python to use the dataframe structure and query on it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f2496",
   "metadata": {},
   "source": [
    "## 21-04-22 Publication of the protocol and review of the other team's protocol\n",
    "We published the second version of the protocol, available at [this link](https://dx.doi.org/10.17504/protocols.io.n92ldz598v5b/v2), after adding some python code explaining how we will connect to the API, thanks to the tests done by Alessandro.\n",
    "\n",
    "I also published on Qeios [my review](https://doi.org/10.32388/MOBTWR) of the team La Chouffe's protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c551b81",
   "metadata": {},
   "source": [
    "## 26-04-22 Revision of the DMP and rebuttal letter\n",
    "We had a team meeting to address the issues expressed by the two reviews of our colleagues about the first version of our DMP. We then wrote together the [second version of the DMP](https://doi.org/10.5281/zenodo.6491184), taking into account their remarks and writing also a [response letter](https://doi.org/10.5281/zenodo.6492439) to these reviews. It allowed us to think again on some parts of the DMP that we didn't understand on the first version, when we did not yet have a clear idea on how to realize the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd59f93",
   "metadata": {},
   "source": [
    "## 03-05-22 Team meeting for writing the software\n",
    "We met with the team to try to extract the data we need for the project in python. We tried a lot of different techniques, and after realizing that with the API it would take too much time to make requests on all of OpenCitations dataset, we downloaded the public data dump in zip files and searched for methods to work with zipped files. There is the library [zipfile.py](https://docs.python.org/3/library/zipfile.html), and we started writing code to extract the first csv from the OpenCitations dump through this library.\n",
    "We also discovered the [vaex library](https://vaex.io/docs/index.html) to process huge quantity of data in a faster way than pandas.\n",
    "\n",
    "Upon doing research on other projects that had to deal with the same quantity of data, I found the [Coronavirus Open Citations Dataset](https://opencitations.github.io/coronavirus/) by Prof. Peroni, and taking a look at [the method](https://github.com/opencitations/coronavirus/blob/master/data.ipynb) he used to extract data, I tried to test out the doaj api to extract the articles from just one journal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474b798",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from json import loads, load, dump\n",
    "import logging\n",
    "\n",
    "print(\"hey\")\n",
    "logging_level = logging.INFO\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s.')\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging_level)\n",
    "\n",
    "dois = set()\n",
    "doaj_query = \"https://doaj.org/api/search/articles/issn:0009-4293\"\n",
    "doaj_data = get(doaj_query)\n",
    "if doaj_data.status_code == 200:\n",
    "    doaj_data.encoding=\"utf-8\"\n",
    "    \n",
    "    doaj_json = loads(doaj_data.text).get(\"results\")\n",
    "    \n",
    "    for item in doaj_json:\n",
    "        if item[\"bibjson\"][\"identifier\"][0][\"type\"] == \"doi\":\n",
    "            print(item[\"bibjson\"][\"identifier\"][0][\"id\"])\n",
    "            dois.add(item[\"bibjson\"][\"identifier\"][0][\"id\"])\n",
    "        else:\n",
    "            print(\"this article doesn't have a doi assigned in doaj api\")\n",
    "print(len(dois))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a5d1d",
   "metadata": {},
   "source": [
    "This is actually only the first 10 of the response but it was just a test to use the api. Nonetheless we can notice that a lot of articles are missing dois and we will need to take it into account wen extracting our data from the dump.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fde65d",
   "metadata": {},
   "source": [
    "## 06-05-22 Team meeting for writing the software and revising the protocol\n",
    "We had a team meeting to try to once again redefine the goals of our research and understand how to handle the quantity of data to be used. We agreed on the data structures to use: firstly we will extract from the articles data dump of DOAJ a JSON where for each journal, we will keep info about the issn and eissn, and a list of all dois of the articles published in the journal. Then, from OpenCitations' data dump we will extract citations involving dois contained in our JSON. \n",
    "\n",
    "I wrote some code for extracting the information we need from the DOAJ dump and creating an json (final version [here](#)\n",
    "\n",
    "We also started writing the response letter to the review by Chiara Catizone of our protocols and started reviewing it. We then split the work to prepare for our next meeting on Tuesday 10-05-22\n",
    "\n",
    "## 09-05-22\n",
    "I continued writing the function and wrote my part of the rebuttal letter and protocol, adding some references for our research. \n",
    "Chiara published [the protocol](https://dx.doi.org/10.17504/protocols.io.n92ldz598v5b/v3) and [the letter](https://doi.org/10.5281/zenodo.6533180). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65ab81",
   "metadata": {},
   "source": [
    "## 10-05-22\n",
    "With the team we met to finish the function above cleaning the data from DOAJ dump, which we wrote with Chiara and Alessandro. \n",
    "During the cleaning phase from DOAJ data dump, we encountered the following problems:\n",
    "In journals dump, there was a journal that appeared twice(issn 2356-2048 and eissn 2356-203X), which we discovered when changing the data structure containing all of the journals issn+eissn from a list to a set.\n",
    "We noticed that there was one element less in the set and thus, that this combination issn+eissn appeared twice.\n",
    "It seems that instead of updating this journal in the dump, they created a duplicate with only a few different info. We removed the duplicate.\n",
    "\n",
    "We noticed some discrepancies between the information extracted from the articles metadata and the journals metadata.\n",
    "Some articles contain only the issn or the eissn, while the journal has both identifiers. Some articles have the same identifier for both, and some articles\n",
    "have an issn and an eissn while the journals metadata has only one of these information.\n",
    "We decided to align with the journals metadata and take it as base information. \n",
    "\n",
    "Here is the final code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a523353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import json\n",
    "\n",
    "# Initializing the set that will contain all of the journals 'issn+eissn'\n",
    "journals=set()\n",
    "\n",
    "# Extracting data from the DOAJ journals dump (May 7th, 2022)\n",
    "tar_journals = tarfile.open(\"doaj_journal_data_2022-05-07.tar.gz\", \"r:gz\")\n",
    "for tarinfo in tar_journals:\n",
    "\n",
    "    ciao = tar_journals.extractfile(tarinfo)\n",
    "    # Extracting the data in json format\n",
    "    p = json.load(ciao)\n",
    "\n",
    "    for journal in p:\n",
    "        # For every journal, extract only the info about issn and eissn.\n",
    "        # Through some tests, we verified that there is always at least one of the two info for each record in the dump.\n",
    "        try:\n",
    "            if journal[\"bibjson\"][\"pissn\"]:\n",
    "                journal_issn = journal[\"bibjson\"][\"pissn\"]\n",
    "        except KeyError:\n",
    "            journal_issn=\"\"\n",
    "        try:\n",
    "            if journal[\"bibjson\"][\"eissn\"]:\n",
    "                journal_eissn = journal[\"bibjson\"][\"eissn\"]\n",
    "        except KeyError:\n",
    "            journal_eissn=\"\"\n",
    "\n",
    "        #Creating our own unique identifier for each journal which is a concatenation of the issn and the eissn\n",
    "        key_dict = f\"{journal_issn}{journal_eissn}\"\n",
    "\n",
    "        journals.add(key_dict)\n",
    "\n",
    "print(\"number of journals: \"+str(len(journals)))\n",
    "\n",
    "\"\"\" For testing with pandas\n",
    "journals_df = pd.DataFrame(journals)\n",
    "journals_df.head()\n",
    "journals_df[0]\n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce57f9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14439\n",
      "[{'last_updated': '2022-02-02T04:06:29Z', 'bibjson': {'identifier': [{'type': 'doi'}, {'id': '1898', 'type': 'publisher'}, {'id': '1875-6883', 'type': 'eissn'}], 'journal': {'volume': '2', 'number': '4', 'country': 'FR', 'issns': ['1875-6883', '1875-6891'], 'publisher': 'Atlantis Press', 'language': ['EN'], 'title': 'International Journal of Computational Intelligence Systems'}, 'month': '12', 'year': '2009', 'subject': [{'code': 'QA75.5-76.95', 'scheme': 'LCC', 'term': 'Electronic computers. Computer science'}], 'author': [{'name': 'Tianrui Li'}, {'name': 'Tianrui Li'}, {'name': 'Tianrui Li'}, {'name': 'Tianrui Li'}, {'name': 'Tianrui Li'}], 'link': [{'content_type': 'PDF', 'type': 'fulltext', 'url': 'https://www.atlantis-press.com/article/1898.pdf'}], 'abstract': '', 'title': 'Foreword'}, 'admin': {'seal': False}, 'created_date': '2019-06-11T20:06:46Z', 'id': '471bd26735f04d67b69543ed852c128f'}, {'last_updated': '2022-02-02T09:43:43Z', 'bibjson': {'identifier': [{'type': 'doi'}, {'id': '2091', 'type': 'publisher'}, {'id': '1875-6883', 'type': 'eissn'}], 'journal': {'volume': '3', 'number': '4', 'country': 'FR', 'issns': ['1875-6883', '1875-6891'], 'publisher': 'Atlantis Press', 'language': ['EN'], 'title': 'International Journal of Computational Intelligence Systems'}, 'month': '10', 'year': '2010', 'subject': [{'code': 'QA75.5-76.95', 'scheme': 'LCC', 'term': 'Electronic computers. Computer science'}], 'author': [{'name': 'F. HERRERA'}, {'name': 'L. MARTÍNEZ'}], 'link': [{'content_type': 'PDF', 'type': 'fulltext', 'url': 'https://www.atlantis-press.com/article/2091.pdf'}], 'abstract': '', 'title': 'Foreword'}, 'admin': {'seal': False}, 'created_date': '2019-07-11T09:46:27Z', 'id': '527beee701f34e4990fdf8ecee7702b0'}, {'last_updated': '2022-03-15T12:14:47Z', 'bibjson': {'identifier': [{'type': 'doi'}, {'id': '13173', 'type': 'publisher'}, {'id': '2352-6386', 'type': 'eissn'}], 'journal': {'volume': '1', 'number': '1', 'country': 'FR', 'issns': ['2352-6386'], 'publisher': 'Atlantis Press', 'language': ['EN'], 'title': 'Journal of Robotics, Networking and Artificial Life (JRNAL)'}, 'month': '6', 'year': '2014', 'subject': [{'code': 'T', 'scheme': 'LCC', 'term': 'Technology'}], 'author': [{'name': 'Masanori Sugisaka'}], 'link': [{'content_type': 'PDF', 'type': 'fulltext', 'url': 'https://www.atlantis-press.com/article/13173.pdf'}], 'abstract': '', 'title': \"Editor's Preface\"}, 'created_date': '2019-07-17T17:46:35Z', 'id': 'df5afd78bb3841b5a45545935b47a929'}, {'last_updated': '2022-04-12T20:38:54Z', 'bibjson': {'identifier': [{'type': 'doi'}, {'id': '2137', 'type': 'publisher'}, {'id': '1875-6883', 'type': 'eissn'}], 'journal': {'volume': '4', 'number': '1', 'country': 'FR', 'issns': ['1875-6883', '1875-6891'], 'publisher': 'Atlantis Press', 'language': ['EN'], 'title': 'International Journal of Computational Intelligence Systems'}, 'month': '2', 'year': '2011', 'subject': [{'code': 'QA75.5-76.95', 'scheme': 'LCC', 'term': 'Electronic computers. Computer science'}], 'author': [{'name': 'Tianrui Li'}, {'name': 'Pawan Lingras'}, {'name': 'Yuefeng Li'}, {'name': 'Joseph Herbert'}], 'link': [{'content_type': 'PDF', 'type': 'fulltext', 'url': 'https://www.atlantis-press.com/article/2137.pdf'}], 'abstract': '', 'title': 'Preface'}, 'admin': {'seal': False}, 'created_date': '2019-07-17T17:46:31Z', 'id': 'e36392c4ec8640a9aecc7eab575ce61a'}, {'last_updated': '2022-04-12T22:53:27Z', 'bibjson': {'identifier': [{'type': 'doi'}, {'id': '2126', 'type': 'publisher'}, {'id': '1875-6883', 'type': 'eissn'}], 'journal': {'volume': '3', 'number': '6', 'country': 'FR', 'issns': ['1875-6883', '1875-6891'], 'publisher': 'Atlantis Press', 'language': ['EN'], 'title': 'International Journal of Computational Intelligence Systems'}, 'month': '12', 'year': '2010', 'subject': [{'code': 'QA75.5-76.95', 'scheme': 'LCC', 'term': 'Electronic computers. Computer science'}], 'author': [{'name': 'Jesus Alcal´a-Fdez'}, {'name': 'Jose M. Alonso'}], 'link': [{'content_type': 'PDF', 'type': 'fulltext', 'url': 'https://www.atlantis-press.com/article/2126.pdf'}], 'abstract': '', 'title': 'Preface'}, 'admin': {'seal': False}, 'created_date': '2019-07-11T09:46:28Z', 'id': 'bd698397b5fa45a8b5b149f3d586e570'}, {'last_updated': '2022-04-12T20:52:33Z', 'bibjson': {'identifier': [{'type': 'doi'}, {'id': '2172', 'type': 'publisher'}, {'id': '1875-6883', 'type': 'eissn'}], 'journal': {'volume': '4', 'number': '3', 'country': 'FR', 'issns': ['1875-6883', '1875-6891'], 'publisher': 'Atlantis Press', 'language': ['EN'], 'title': 'International Journal of Computational Intelligence Systems'}, 'month': '5', 'year': '2011', 'subject': [{'code': 'QA75.5-76.95', 'scheme': 'LCC', 'term': 'Electronic computers. Computer science'}], 'author': [{'name': 'Wuhong Wang'}, {'name': 'Klaus Bengler'}], 'link': [{'content_type': 'PDF', 'type': 'fulltext', 'url': 'https://www.atlantis-press.com/article/2172.pdf'}], 'abstract': '', 'title': 'Foreword'}, 'admin': {'seal': False}, 'created_date': '2019-07-17T17:46:32Z', 'id': '934bdb12424f435eaa37223bc4fd75cb'}]\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the data structure that will be stored in the JSON containing all of the dois of the articles for each journal.\n",
    "# It is of the form {\"issnj1+eissnj1\":{\"title\":title1, \"issn\":issnj1, \"eissn\":eissnj1, \"dois:[\"doi1\", \"doi2\",...]\"},\n",
    "#                   \"issnj2+eissnj2\":{\"title\":title2, \"issn\":issnj2, \"eissn\":eissnj2, \"dois:[\"doi1\", \"doi2\",...]\"}\n",
    "doi_json = {}\n",
    "\n",
    "# Storing the articles that don't have a doi and have been wrongly registered\n",
    "art_without_doi= []\n",
    "\n",
    "# Counting all of the articles\n",
    "num_art=0\n",
    "# Extracting data from the DOAJ articles dump (May 1st, 2022)\n",
    "tar = tarfile.open(\"doaj_article_data_2022-05-01.tar.gz\", \"r:gz\")\n",
    "for tarinfo in tar:\n",
    "\n",
    "    ciao = tar.extractfile(tarinfo)\n",
    "    # Extracting the data in json format\n",
    "    p = json.load(ciao)\n",
    "    for article in p:\n",
    "        num_art+=1\n",
    "        # Initializing the variables as empty strings for each iteration\n",
    "        journal_issn=\"\"\n",
    "        journal_eissn=\"\"\n",
    "        art_doi=\"\"\n",
    "        key_dict=\"\"\n",
    "\n",
    "        # Collecting data for issn, eissn and doi for each article\n",
    "        for el in article[\"bibjson\"][\"identifier\"]:\n",
    "            if el[\"type\"] == \"pissn\":\n",
    "                journal_issn = el[\"id\"]\n",
    "            if el[\"type\"] == \"eissn\":\n",
    "                journal_eissn = el[\"id\"]\n",
    "            if el[\"type\"] == \"doi\" or el[\"type\"] == \"DOI\":\n",
    "                try:\n",
    "                    art_doi = el[\"id\"]\n",
    "                # Some articles had an identifier of type doi registered without inserting the doi, which caused an error\n",
    "                # By setting the art_doi to an empty string, these articles will be added to the list of articles without dois.\n",
    "                except KeyError:\n",
    "                    art_doi = \"\"\n",
    "\n",
    "        # If the article doesn't have any doi registered, it is added to a list which will be later dumped to a json \"articles_without_doi.json\"\n",
    "        if art_doi==\"\":\n",
    "            art_without_doi.append(article)\n",
    "        else:\n",
    "            # Collecting the title of the journal\n",
    "            journal_title=article[\"bibjson\"][\"journal\"][\"title\"]\n",
    "\n",
    "            # Creating our own unique identifier for each journal\n",
    "            key_dict = f\"{journal_issn}{journal_eissn}\"\n",
    "\n",
    "            # Handling cases where the issn and/or eissn from the articles dump don't match the journals dump\n",
    "            if key_dict not in journals:\n",
    "\n",
    "                # Aligning with the journals metadata if there is only the issn registered\n",
    "                if journal_issn in journals:\n",
    "                    key_dict = journal_issn\n",
    "                # Aligning with the journals metadata if there is only the eissn registered\n",
    "                elif journal_eissn in journals:\n",
    "                    key_dict = journal_eissn\n",
    "                else:\n",
    "                    # In case the articles metadata has only one of the 2 identifiers, we check on all of the journals identifiers\n",
    "                    # if the string contains the (e)issn held in the article metadata\n",
    "                    for issn in journals:\n",
    "                        if journal_issn != \"\" and journal_issn in issn:\n",
    "                            key_dict = issn\n",
    "                            break\n",
    "                        elif journal_eissn !=\"\" and journal_eissn in issn:\n",
    "                            key_dict = issn\n",
    "                            break\n",
    "\n",
    "            # Once all of the information are collected, we add them to our final json, adding a new key if it doesn't exist or adding it to the list of dois for the journal\n",
    "            if key_dict in doi_json:\n",
    "                doi_json[key_dict][\"dois\"].append(art_doi)\n",
    "            else:\n",
    "                doi_json[key_dict]={\"title\":journal_title, \"pissn\":journal_issn, \"eissn\":journal_eissn, \"dois\":[art_doi]}\n",
    "\n",
    "print(\"number of journals that have articles with dois: \"+str(len(doi_json)))\n",
    "print(\"number of articles that don't have a doi: \"+str(len(art_without_doi)))\n",
    "print(\"total number of articles processed: \"+str(num_art))\n",
    "\n",
    "# Save a json file with all journals and DOIs\n",
    "with open('doi.json', 'w', encoding='utf8') as json_file:\n",
    "    json.dump(doi_json, json_file, ensure_ascii=False)\n",
    "\n",
    "# Save a json file with the articles that don't have a DOI\n",
    "with open('articles_without_doi.json', 'w', encoding='utf-8') as json_file2:\n",
    "    json.dump(art_without_doi, json_file2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082763d7",
   "metadata": {},
   "source": [
    "Using pandas and comparing the 2 dataframe containing info about the journals and the same info but extracted from the metadata of the articles, we saw that there are no cases where there is neither issn nor eissn.\n",
    "\n",
    "Test with pandas to compare info extracted from journals metadata and article metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dois = pd.read_json('doi.json', orient='index')\n",
    "dois.drop(['dois','title','pissn', 'eissn'],axis=1, inplace=True)\n",
    "dois.reset_index(inplace=True)\n",
    "journal = pd.DataFrame(journals, columns=['index'])\n",
    "\n",
    "\n",
    "new_df = dois.merge(journal, on=['index'], how='left', indicator=True)\n",
    "new_df[new_df['_merge'] == 'left_only']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
